import time
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_ollama import OllamaLLM
from LLMquery.prompts.build_prompt import build_prompt  # ‚úÖ new central router
from LLMquery.prompts.math_utils import evaluate_math

# ============================================================
# 1Ô∏è‚É£ Initialize FastAPI app
# ============================================================
app = FastAPI(title="LoanDocQA+ API (Multi-Mode Prompting + SymPy Evaluator)", version="5.0")

# ‚úÖ Allow cross-origin access
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================================
# 2Ô∏è‚É£ Configuration
# ============================================================
CHROMA_PATH = "LLMquery/vectorstores/loan_doc_index"
EMBED_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
LLM_MODEL = "phi3"   # üîÅ general reasoning model
MAX_TOKENS = 512
MEMORY_LIMIT = 5

# ============================================================
# 3Ô∏è‚É£ Load embeddings, vectorstore, and retriever
# ============================================================
print("üìÇ Loading Chroma vectorstore and Embeddings...")
embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL)
vectorstore = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings)
retriever = vectorstore.as_retriever(search_kwargs={"k": 2})
print(f"‚úÖ Loaded Chroma DB with {vectorstore._collection.count()} entries.\n")

# ============================================================
# 4Ô∏è‚É£ Initialize LLM (direct reasoning mode)
# ============================================================
print(f"üß† Loading LLM model: {LLM_MODEL}")
llm = OllamaLLM(model=LLM_MODEL, temperature=0.2, num_predict=MAX_TOKENS)
print("‚úÖ LLM loaded successfully.\n")
print("‚öôÔ∏è Running in direct reasoning mode (multi-prompt architecture).\n")

# ============================================================
# 5Ô∏è‚É£ In-memory chat history
# ============================================================
conversation_history = []  # [{"role": "user"/"assistant", "content": "..."}]

def add_to_history(role: str, content: str):
    """Store conversation turns and keep memory limited."""
    conversation_history.append({"role": role, "content": content})
    if len(conversation_history) > MEMORY_LIMIT * 2:
        del conversation_history[:2]

def get_history_text():
    """Return formatted conversation memory for context."""
    if not conversation_history:
        return ""
    return "\n\n".join(
        f"{msg['role'].upper()}: {msg['content']}" for msg in conversation_history
    )

# ============================================================
# 6Ô∏è‚É£ Root endpoint
# ============================================================
@app.get("/")
def home():
    return {
        "message": "‚úÖ LoanDocQA+ API (Multi-Prompt + SymPy Evaluator) is running.",
        "vectorstore_size": vectorstore._collection.count(),
        "model": LLM_MODEL,
    }

# ============================================================
# 7Ô∏è‚É£ Query endpoint ‚Äî multi-prompt, math-verified
# ============================================================
@app.post("/query_stream")
async def query_stream(request: Request):
    """Answer user queries using smart prompt routing + math verification."""
    data = await request.json()
    question = data.get("question", "").strip()
    mode = data.get("mode", None)  # ‚úÖ allow explicit mode override
    if not question:
        return {"answer": "Please provide a valid question."}

    start_time = time.time()
    print(f"üß† Query received: {question}")
    if mode:
        print(f"üéØ Mode override: {mode}")

    # Step 1Ô∏è‚É£ Retrieve relevant docs
    docs = retriever.get_relevant_documents(question)
    print(f"üìÑ Retrieved {len(docs)} docs")

    # Step 2Ô∏è‚É£ Build prompt dynamically (auto or manual mode)
    prompt = build_prompt(question, docs, mode)
    print("üß© Prompt built successfully using adaptive logic.")

    # Step 3Ô∏è‚É£ Generate response from LLM
    try:
        response = llm.invoke(prompt)
    except Exception as e:
        print(f"‚ö†Ô∏è LLM execution failed: {e}")
        response = "An error occurred while processing your query."

    # Step 4Ô∏è‚É£ Evaluate math expressions for verification (finance mode)
    computed_results = evaluate_math(response)
    if computed_results:
        response += "\n\nüí° **Verified Computation:**\n" + "\n".join(
            f"- {r}" for r in computed_results
        )

    # Step 5Ô∏è‚É£ Update memory
    add_to_history("user", question)
    add_to_history("assistant", response)

    # Step 6Ô∏è‚É£ Prepare source snippets for UI
    sources = []
    for doc in docs[:3]:
        snippet = doc.page_content.strip().replace("\n", " ")
        snippet = snippet[:400] + "..." if len(snippet) > 400 else snippet
        file_name = (
            doc.metadata.get("source")
            or doc.metadata.get("file")
            or "unknown_source"
        )
        sources.append({"file": file_name, "snippet": snippet})

    # Step 7Ô∏è‚É£ Timing + Return
    time_taken = round(time.time() - start_time, 2)
    print(f"‚úÖ Completed in {time_taken}s | Answer length: {len(response)}")

    return {
        "answer": response,
        "sources": sources,
        "time_taken_sec": time_taken,
        "mode_used": mode or "auto",  # ‚úÖ expose mode to frontend
        "memory_turns": len(conversation_history),
    }
